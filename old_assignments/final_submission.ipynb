{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AML Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitted by:-\n",
    "\n",
    "Subhadutta Mahapatra (MDS201934)\n",
    "\n",
    "Tanmey Rawal(MDS201938)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install this library to see visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.7.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_track(file_name):\n",
    "    track=open(file_name,'r')\n",
    "    temp = track.read() \n",
    "    temp_list = temp.split(\"\\n\") \n",
    "    string_track=[t.replace('F', '5').replace('S', '0').replace('O', '2').replace('X','1') for t in temp_list]\n",
    "    final_track=[]\n",
    "    for each in string_track:\n",
    "        temporary=[]\n",
    "        if len(each)!=0:\n",
    "            for i in each:\n",
    "                temporary.append(int(i))\n",
    "            final_track.append(temporary)\n",
    "    return np.array(final_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "track=get_track('track-1.txt')\n",
    "track=np.flip(track)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Track is flipped as I want starting of array to be the start of agent's position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1],\n",
       "       [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS,COLS=track.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    \n",
    "    def get_start_line(self):\n",
    "        self.start_line = np.array([np.array([ROWS-1,j]) for j in range(COLS) if self.track[ROWS-1,j] == 5])\n",
    "        \n",
    "    def get_finish_line(self):\n",
    "        \n",
    "        self.finish_line = np.array([np.array([0,i]) for i in range(COLS) if self.track[0,i] == 0])\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.track = track\n",
    "        self.get_start_line()\n",
    "        self.get_finish_line()\n",
    "        self.load_Q_vals()\n",
    "        self.load_C_vals()\n",
    "        self.load_pi()\n",
    "        self.load_rewards()\n",
    "        self.df = 0.99\n",
    "        self.eps = 0.2\n",
    "        self.episode = dict({'state':[],'action':[],'probs':[],'reward':[None]})\n",
    "        \n",
    "    def save_rewards(self,filename = 'rewards'):\n",
    "        self.rewards = np.array(self.rewards)\n",
    "        np.save(filename,self.rewards)\n",
    "        self.rewards = list(self.rewards)\n",
    "        \n",
    "    def load_rewards(self):\n",
    "        self.rewards = list(np.load('rewards.npy'))\n",
    "        \n",
    "    def save_pi(self,filename = 'pi.npy'):\n",
    "        np.save(filename,self.pi)\n",
    "        \n",
    "    def load_pi(self):\n",
    "        self.pi = np.load('pi.npy')\n",
    "        \n",
    "    def save_C_vals(self,filename = 'C_vals.npy'):\n",
    "        np.save(filename,self.C_vals)\n",
    "        \n",
    "    def load_C_vals(self):\n",
    "        self.C_vals = np.load('C_vals.npy')\n",
    "        \n",
    "    def save_Q_vals(self,filename = 'Q_vals.npy'):\n",
    "        np.save(filename,self.Q_vals)\n",
    "        \n",
    "    def load_Q_vals(self):\n",
    "        self.Q_vals = np.load('Q_vals.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def get_new_state(self, state, action):\n",
    "        new_state = state.copy()\n",
    "        new_state[0] = state[0] - state[2]\n",
    "        new_state[1] = state[1] + state[3]\n",
    "        new_state[2] = state[2] + action[0]\n",
    "        new_state[3] = state[3] + action[1]\n",
    "        return new_state\n",
    "    \n",
    "    def select_randomly(self,arr):\n",
    "        return np.random.choice(arr)\n",
    "    \n",
    "    def set_zero(arr):\n",
    "        arr[:] = 0\n",
    "        return arr\n",
    "    \n",
    "    def is_finish_line_crossed(self, state, action):\n",
    "        new_state = self.get_new_state(state, action)\n",
    "        old_postion, new_position = state[0:2], new_state[0:2]\n",
    "        \n",
    "        rows = np.array(range(new_position[0],old_postion[0]+1))\n",
    "        cols = np.array(range(old_postion[1],new_position[1]+1))\n",
    "        fin = set([tuple(x) for x in self.data.finish_line])\n",
    "        row_col_matrix = [(x,y) for x in rows for y in cols]\n",
    "        intersect = [x for x in row_col_matrix if x in fin]\n",
    "        \n",
    "        return len(intersect) > 0\n",
    "    \n",
    "    def has_collided(self, new_state,old_state):  \n",
    "        sub_track=self.data.track[min(old_state[0],new_state[0]):max(old_state[0],new_state[0])+1,min(old_state[1],new_state[1]):max(old_state[1],new_state[1])+1]\n",
    "        if 1 in sub_track:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def is_out_of_track(self, state, action):\n",
    "        new_state = self.get_new_state(state, action)\n",
    "        old_postion, new_position = state[0:2], new_state[0:2]\n",
    "        \n",
    "        if new_position[0] < 0 or new_position[1] < 0 or new_position[1] >= COLS:\n",
    "            return True\n",
    "        elif self.has_collided(new_position,old_postion):\n",
    "            return True\n",
    "        else:\n",
    "            return self.data.track[tuple(new_position)] == 1\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.data.episode = dict({'state':[],'action':[],'probs':[],'reward':[None]})\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def start(self):\n",
    "        state = np.zeros(4,dtype='int')\n",
    "        state[0] = ROWS-1\n",
    "        state[1] = self.select_randomly(self.data.start_line[:,1])\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        self.data.episode['action'].append(action)\n",
    "        reward = -1\n",
    "        reward_obs = -10\n",
    "        reward_finish = 10\n",
    "        \n",
    "        if (self.is_finish_line_crossed(state, action)):\n",
    "            new_state = self.get_new_state(state, action)\n",
    "            \n",
    "            self.data.episode['reward'].append(reward_finish)\n",
    "            self.data.episode['state'].append(new_state)\n",
    "            self.step_count += 1\n",
    "            \n",
    "            return None, new_state\n",
    "            \n",
    "        elif (self.is_out_of_track(state, action)):\n",
    "            self.data.episode['reward'].append(reward_obs)\n",
    "            new_state = self.start()\n",
    "        else:\n",
    "            new_state = self.get_new_state(state, action)\n",
    "        \n",
    "        self.data.episode['reward'].append(reward)\n",
    "        self.data.episode['state'].append(new_state)\n",
    "        self.step_count += 1\n",
    "        \n",
    "        return reward, new_state\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def possible_actions(self, velocity):\n",
    "        action_space = [(-1,-1),(-1,0),(0,-1),(-1,1),(0,0),(1,-1),(0,1),(1,0),(1,1)]\n",
    "        action_space=[np.array(x) for x in action_space]\n",
    "        pa = []\n",
    "        for i,x in zip(range(9),action_space):\n",
    "            new_vel = np.add(velocity,x)\n",
    "            if (new_vel[0] < 5) and (new_vel[1] < 5) and ~(new_vel[0] == 0 and new_vel[1] == 0):\n",
    "                pa.append(i)\n",
    "        pa = np.array(pa)\n",
    "        \n",
    "        return pa\n",
    "    \n",
    "    def map_to_1D(self,action):\n",
    "        action_space = [(-1,-1),(-1,0),(0,-1),(-1,1),(0,0),(1,-1),(0,1),(1,0),(1,1)]\n",
    "        for i,x in zip(range(9),action_space):\n",
    "            if action[0]==x[0] and action[1]==x[1]:\n",
    "                return i\n",
    "    \n",
    "    def map_to_2D(self,action):\n",
    "        act = [(-1,-1),(-1,0),(0,-1),(-1,1),(0,0),(1,-1),(0,1),(1,0),(1,1)]\n",
    "        return act[action]\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_action(self, state, policy):\n",
    "        return self.map_to_2D(policy(state, self.possible_actions(state[2:4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Monte_Carlo_Control:\n",
    "    \n",
    "    def evaluate_target_policy(self):\n",
    "        env.reset()\n",
    "        state = env.start()\n",
    "        self.data.episode['state'].append(state)\n",
    "        rew = -1\n",
    "        while rew!=None:\n",
    "            action = agent.get_action(state,self.generate_target_policy_action)\n",
    "            rew, state = env.step(state,action)\n",
    "            \n",
    "        self.data.rewards.append(sum(self.data.episode['reward'][1:]))\n",
    "        \n",
    "    \n",
    "    def plot_rewards(self):\n",
    "        ax, fig = plt.subplots(figsize=(20,20))\n",
    "        x = np.arange(1,len(self.data.rewards)+1)\n",
    "        plt.plot(x*10, self.data.rewards, linewidth=0.5, color = '#BB0FC0') \n",
    "        plt.xlabel('Episode number', size = 10)\n",
    "        plt.ylabel('Reward',size = 10)\n",
    "        plt.title('Plot of Reward vs Episode Number',size=20)\n",
    "        plt.xticks(size=10)\n",
    "        plt.yticks(size=10)\n",
    "        plt.savefig('rewardgraph.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def save_your_work(self):\n",
    "        self.data.save_Q_vals()\n",
    "        self.data.save_C_vals()\n",
    "        self.data.save_pi()\n",
    "        self.data.save_rewards()\n",
    "    \n",
    "    def determine_probability_behaviour(self, state, action, possible_actions):\n",
    "        best_action = self.data.pi[tuple(state)]\n",
    "        num_actions = len(possible_actions)\n",
    "        \n",
    "        if best_action in possible_actions:\n",
    "            if action == best_action:\n",
    "                prob = 1 - self.data.eps + self.data.eps/num_actions\n",
    "            else:\n",
    "                prob = self.data.eps/num_actions\n",
    "        else:\n",
    "            prob = 1/num_actions\n",
    "        \n",
    "        self.data.episode['probs'].append(prob)\n",
    "    \n",
    "    def generate_target_policy_action(self, state, possible_actions):\n",
    "        \n",
    "        if self.data.pi[tuple(state)] in possible_actions:\n",
    "            action = self.data.pi[tuple(state)]\n",
    "        else:\n",
    "            action = np.random.choice(possible_actions)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def generate_behavioural_policy_action(self, state, possible_actions):\n",
    "        if np.random.rand() > self.data.eps and self.data.pi[tuple(state)] in possible_actions:\n",
    "            action = self.data.pi[tuple(state)]\n",
    "        else:\n",
    "            action = np.random.choice(possible_actions)\n",
    "        \n",
    "        self.determine_probability_behaviour(state, action, possible_actions)\n",
    "    \n",
    "        return action\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        for i in range(ROWS):\n",
    "            for j in range(COLS):\n",
    "                if self.data.track[i,j]!=1:\n",
    "                    for k in range(5):\n",
    "                        for l in range(5):\n",
    "                            self.data.pi[i,j,k,l] = np.argmax(self.data.Q_vals[i,j,k,l])\n",
    "    \n",
    "    def control(self,env,agent):\n",
    "        env.reset()\n",
    "        state = env.start()\n",
    "        self.data.episode['state'].append(state)\n",
    "        rew = -1\n",
    "        while rew!=None:\n",
    "            action = agent.get_action(state,self.generate_behavioural_policy_action)\n",
    "            rew, state = env.step(state,action)\n",
    "        \n",
    "        G = 0\n",
    "        W = 1\n",
    "        T = env.step_count\n",
    "        for t in range(T-1,-1,-1):\n",
    "            G = data.df * G + self.data.episode['reward'][t+1]\n",
    "            S_t = tuple(self.data.episode['state'][t])\n",
    "            A_t = agent.map_to_1D(self.data.episode['action'][t])\n",
    "            \n",
    "            S_list = list(S_t)\n",
    "            S_list.append(A_t)\n",
    "            SA = tuple(S_list)\n",
    "            \n",
    "            self.data.C_vals[SA] += W\n",
    "            self.data.Q_vals[SA] += (W*(G-self.data.Q_vals[SA]))/(self.data.C_vals[SA])           \n",
    "            self.data.pi[S_t] = np.argmax(self.data.Q_vals[S_t])\n",
    "            if A_t!=self.data.pi[S_t]:\n",
    "                break\n",
    "            W /= self.data.episode['probs'][t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    \n",
    "    \n",
    "    def visualize_episode():\n",
    "        for i in range(self.data.episode['state']):\n",
    "            vis.visualize_racetrack(i)\n",
    "    \n",
    "    def create_window(self):\n",
    "        self.display = pygame.display.set_mode((self.width, self.height))\n",
    "        pygame.display.set_caption(\"Racetrack\")\n",
    "    \n",
    "    def setup(self):\n",
    "        self.cell_edge = 25\n",
    "        self.width = COLS*self.cell_edge\n",
    "        self.height = ROWS*self.cell_edge\n",
    "        self.create_window()\n",
    "        self.window = True\n",
    "\n",
    "    def close_window(self):\n",
    "        self.window = False\n",
    "        pygame.quit()\n",
    "\n",
    "    def draw(self, state = np.array([])):\n",
    "        self.display.fill(0)\n",
    "        for i in range(ROWS):\n",
    "            for j in range(COLS):\n",
    "                if self.data.track[i,j]!= 1:\n",
    "                    if self.data.track[i,j] == 2:\n",
    "                        color = (255,0,0)\n",
    "                    elif self.data.track[i,j] == 5:\n",
    "                        color = (255,255,0)\n",
    "                    elif self.data.track[i,j] == 0:\n",
    "                        color = (0,255,0)\n",
    "                    pygame.draw.rect(self.display,color,((j*self.cell_edge,i*self.cell_edge),(self.cell_edge,self.cell_edge)),1)\n",
    "        \n",
    "        if len(state)>0:\n",
    "            pygame.draw.rect(self.display,(255,255,255),((state[1]*self.cell_edge,state[0]*self.cell_edge),(self.cell_edge,self.cell_edge)),0)\n",
    "        \n",
    "        pygame.display.update()\n",
    "        \n",
    "        global count\n",
    "        \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.loop = False\n",
    "                self.close_window()\n",
    "                return 'stop'\n",
    "            elif event.type == pygame.KEYDOWN and event.key == pygame.K_SPACE:\n",
    "                count += 1\n",
    "                self.loop = False\n",
    "                \n",
    "        return None\n",
    "        \n",
    "    def visualize_racetrack(self, state = np.array([])):\n",
    "        if self.window == False:\n",
    "            self.setup()\n",
    "        self.loop = True\n",
    "        while(self.loop):\n",
    "            ret = self.draw(state)\n",
    "            if ret!=None:\n",
    "                return ret\n",
    "    \n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.window = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = Data()\n",
    "env = Environment(data)\n",
    "mcc = Monte_Carlo_Control(data)\n",
    "vis = Visualizer(data)\n",
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis.visualize_racetrack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'action_space' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-9ecab4e4808c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmcc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mmcc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_target_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-8398abffd55b>\u001b[0m in \u001b[0;36mcontrol\u001b[1;34m(self, env, agent)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mG\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reward'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mS_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0mA_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_to_1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'action'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mS_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-5ec5a2bbdce9>\u001b[0m in \u001b[0;36mmap_to_1D\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmap_to_1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'action_space' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    mcc.control(env,agent)\n",
    "    if i%10 == 9:\n",
    "        mcc.evaluate_target_policy()\n",
    "    \n",
    "    if i%100 == 99:\n",
    "        mcc.save_your_work()\n",
    "        mcc.plot_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = 50\n",
    "S = sum(data.rewards[:ch])/ch\n",
    "\n",
    "R = []\n",
    "\n",
    "for i in range(ch,len(data.rewards)):\n",
    "    R.append(S)\n",
    "    S *= ch\n",
    "    S += data.rewards[i]\n",
    "    S -= data.rewards[i-ch]\n",
    "    S /= ch\n",
    "\n",
    "ax, fig = plt.subplots(figsize=(60,30))\n",
    "x = np.arange(1,len(R)+1)\n",
    "plt.plot(x*10, R, linewidth=1, color = '#BB8FCE')\n",
    "plt.xlabel('Episode number', size = 40)\n",
    "plt.ylabel('Reward',size = 40)\n",
    "plt.title('Plot of Reward vs Episode Number',size=40)\n",
    "plt.xticks(size=40)\n",
    "plt.yticks(size=40)\n",
    "plt.savefig('RewardGraph2.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "env.reset()\n",
    "state = env.start()\n",
    "mcc.data.episode['state'].append(state)\n",
    "rew = -1\n",
    "while rew!=None:\n",
    "    action = agent.get_action(state,mcc.generate_target_policy_action)\n",
    "    rew, state = env.step(state,action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.episode['state']:\n",
    "    print(\"Press space to see the block moving. You would have to keep pressing if you want to see the movement\")\n",
    "    if vis.visualize_racetrack(i) == 'stop':\n",
    "        break\n",
    "vis.close_window()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
