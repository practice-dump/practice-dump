{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core data structure in Spark is a resilient distributed data set (RDD). As the name suggests, an RDD is Spark's representation of a data set that's distributed across the RAM, or memory, of a cluster of many machines. An RDD object is essentially a collection of elements we can use to hold lists of tuples, dictionaries, lists, etc. Similar to a pandas DataFrame, we can load a data set into an RDD, and then run any of the methods accesible to that object.\n",
    "\n",
    "PySpark\n",
    "\n",
    "While the Spark toolkit is in Scala, a language that compiles down to bytecode for the JVM, the open source community has developed a wonderful toolkit called PySpark that allows us to interface with RDDs in Python. Thanks to a library called Py4J, Python can interface with Java objects (in our case RDDs). Py4J is also one of the tools that makes PySpark work.\n",
    "\n",
    "In this mission, we'll work with a data set containing the names of all of the guests who have appeared on The Daily Show.\n",
    "\n",
    "To start off, we'll load the data set into an RDD. We're using the TSV version of FiveThirtyEight's data set. TSV files use a tab character (\"\\t\") as the delimiter, instead of the comma (\",\") that CSV files use.\n",
    "\n",
    "['YEAR\\tGoogleKnowlege_Occupation\\tShow\\tGroup\\tRaw_Guest_List',\n",
    " '1999\\tactor\\t1/11/99\\tActing\\tMichael J. Fox',\n",
    " '1999\\tComedian\\t1/12/99\\tComedy\\tSandra Bernhard',\n",
    " '1999\\ttelevision actress\\t1/13/99\\tActing\\tTracey Ullman',\n",
    " '1999\\tfilm actress\\t1/14/99\\tActing\\tGillian Anderson']\n",
    " \n",
    "textFile is a method of a org.apache.spark.SparkContext class that reads a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI, and return it as an RDD of Strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-56777e7d266b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraw_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"daily_show.tsv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "raw_data = sc.textFile(\"daily_show.tsv\")\n",
    "raw_data.take(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark, the SparkContext object manages the connection to the clusters, and coordinates the running of processes on those clusters. More specifically, it connects to the cluster managers. The cluster managers control the executors that run the computations. Here's a diagram from the Spark documentation that will help you visualize the architecture:\n",
    "\n",
    " ![title](cluster.png)\n",
    " \n",
    "We automatically have access to the SparkContext object sc. We then run the following code to read the TSV data set into an RDD object raw_data:\n",
    "\n",
    "raw_data = sc.textFile(\"daily_show.tsv\")\n",
    "\n",
    "The RDD object raw_data closely resembles a list of string objects, with one object for each line in the data set. We then use the take() method to print the first five elements of the RDD:\n",
    "\n",
    "raw_data.take(5)\n",
    "\n",
    "To explore the other methods an RDD object has access to, check out the PySpark documentation. take(n) will return the first n elements of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You may be wondering why, if an RDD resembles a Python list, we don't just use bracket notation to access elements in the RDD.\n",
    "\n",
    "The answer is that Spark distributes RDD objects across many partitions, and the RDD object is specifically designed to handle distributed data. We can't rely on the standard implementation of a list for these reasons.\n",
    "\n",
    "Spark offers many advantages over regular Python, though. For example, thanks to RDD abstraction, you can run Spark locally on your own computer. Spark will simulate distributing your calculations over many machines by automatically slicing your computer's memory into partitions.\n",
    "\n",
    "Spark's RDD implementation also lets us evaluate code \"lazily,\" meaning we can postpone running a calculation until absolutely necessary. On the previous screen, Spark waited to load the TSV file into an RDD until raw_data.take(5) executed. When our code called raw_data = sc.textFile(\"dail_show.tsv\"), Spark created a pointer to the file, but didn't actually read it into raw_data until raw_data.take(5) needed that variable to run its logic.\n",
    "\n",
    "The advantage of \"lazy\" evaluation is that we can build up a queue of tasks and let Spark optimize the overall workflow in the background. In regular Python, the interpreter can't do much workflow optimization. We'll see more examples of lazy evaluation later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Spark borrowed heavily from Hadoop's MapReduce pattern, it's still quite different in many ways. \n",
    "\n",
    "The key idea to understand when working with Spark is data pipelining. Every operation or calculation in Spark is essentially a series of steps that we can chain together and run in succession to form a pipeline. Each step in the pipeline returns either a Python value (such as an integer), a Python data structure (such as a dictionary), or an RDD object.\n",
    "\n",
    "We'll start with the map() function.\n",
    "\n",
    "Map()\n",
    "\n",
    "The map(f) function applies the function f to every element in the RDD. Because RDDs are iterable objects (like most Python objects), Spark runs function f on each iteration and returns a new RDD.\n",
    "\n",
    "We'll walk through an example of a map function so you can get a better sense of how it works. If you look carefully, you'll see that raw_data is in a format that's hard to work with. While the elements are currently all strings, we'd like to convert each of them into a list to make the data more manageable. To do this the traditional way, we would:\n",
    "\n",
    "1. Use a 'for' loop to iterate over the collection\n",
    "2. Split each `string` on the delimiter\n",
    "3. Store the result in a `list`\n",
    "\n",
    "Let's see how we can use map to do this with Spark instead.\n",
    "\n",
    "In the code cell:\n",
    "\n",
    "1. Call the RDD function `map()` to specify we want to apply the logic in the parentheses to every line in our data set.\n",
    "2. Write a lambda function that splits each line using the tab delimiter (\\t), and assign the resulting RDD to `daily_show`.\n",
    "3. Call the RDD function `take()` on `daily_show` to display the first five elements (or rows) of the resulting RDD.\n",
    "\n",
    "We call the map(f) function a transformation step. It requires either a named or lambda function f.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "daily_show.take(5)\n",
    "# Hit run to see the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the wonderful features of PySpark is the ability to separate our logic - which we prefer to write in Python - from the actual data transformation. In the previous code cell, we wrote this lambda function in Python code:\n",
    "\n",
    "raw_data.map(lambda line: line.split('\\t'))\n",
    "\n",
    "Even though the function was in Python, we also took advantage of Scala when Spark actually ran the code over our RDD. This is the power of PySpark. Without learning any Scala, we get to harness the data processing performance gains from Spark's Scala architecture. Even better, when we ran the following code, it returned the results to us in Python-friendly notation:\n",
    "\n",
    "daily_show.take(5)\n",
    "\n",
    "Transformations and Actions\n",
    "\n",
    "There are two types of methods in Spark:\n",
    "\n",
    "1. Transformations - map(), reduceByKey()\n",
    "2. Actions - take(), reduce(), saveAsTextFile(), collect()\n",
    "\n",
    "Transformations are lazy operations that always return a reference to an RDD object. Spark doesn't actually run the transformations, though, until an action needs to use the RDD resulting from a transformation. Any function that returns an RDD is a transformation, and any function that returns a value is an action. These concepts will become more clear as we work through this lesson and practice writing PySpark code.\n",
    "\n",
    "Immutability\n",
    "\n",
    "You may be wondering why we couldn't just split each string in place, instead of creating a new object daily_show. In Python, we could have modified the collection element-by-element in place, without returning and assigning the results to a new object.\n",
    "\n",
    "RDD objects are immutable, meaning that we can't change their values once we've created them. In Python, list and dictionary objects are mutable (we can change their values), while tuple objects are immutable. The only way to modify a tuple object in Python is to create a new tuple object with the necessary updates. \n",
    "\n",
    "\n",
    "Spark uses the immutability of RDDs to enhance calculation speeds. The mechanics of how it does this is complex. HOW?\n",
    "https://www.quora.com/Why-is-RDD-immutable-in-Spark\n",
    "1. Immutability rules out a big set of potential problems due to updates from multiple threads at once. Immutable data is definitely safe to share across processes.\n",
    "\n",
    "2. They're not just immutable but a deterministic function of their input. This plus immutability also means the RDD's parts can be recreated at any time. This makes caching, sharing and replication easy.\n",
    "\n",
    "3. These are significant design wins, at the cost of having to copy data rather than mutate it in place. Generally, that's a decent tradeoff to make: gaining the fault tolerance and correctness with no developer effort is worth spending memory and CPU on, since the latter are cheap.\n",
    "\n",
    "4. A corollary: immutable data can as easily live in memory as on disk. This makes it reasonable to easily move operations that hit disk to instead use data in memory, and again, adding memory is much easier than adding I/O bandwidth.\n",
    "\n",
    "5. Of course, an RDD isn't really a collection of data, but just a recipe for making data from other data. It is not literally computed by materializing every RDD completely. That is, a lot of the \"copy\" can be optimized away too.\n",
    "\n",
    "6. The idea of an RDD has no origin in MapReduce.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to tally up the number of guests who have appeared on The Daily Show during each year. If daily_show were a list of lists, we could write the following Python code to achieve this result:\n",
    "\n",
    "The keys in tally will be the years, and the values will be the totals for the number of lines associated with each year.\n",
    "\n",
    "To achieve the same result with Spark, we'll have to use a Map step, then a ReduceByKey step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tally = dict()\n",
    "for line in daily_show:\n",
    "  year = line[0]\n",
    "  if year in tally.keys():\n",
    "    tally[year] = tally[year] + 1\n",
    "  else:\n",
    "    tally[year] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apache Spark Code\n",
    "\n",
    "tally = daily_show.map(lambda x: (x[0], 1)).reduceByKey(lambda x,y: x+y)\n",
    "print(tally)\n",
    "\n",
    "# Output --> PythonRDD[10] at RDD at PythonRDD.scala:48\n",
    "\n",
    "# Variables\n",
    "\"\"\"tallyPipelinedRDD (<class 'pyspark.rdd.PipelinedRDD'>)\n",
    "PythonRDD[10] at RDD at PythonRDD.scala:48\n",
    " daily_showPipelinedRDD (<class 'pyspark.rdd.PipelinedRDD'>)\n",
    "PythonRDD[11] at RDD at PythonRDD.scala:48\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that printing tally didn't return the histogram we were hoping for. Because of lazy evaluation, PySpark delayed executing the map and reduceByKey steps until we actually need them. Before we use take() to preview the first few elements in tally, we'll walk through the code we just wrote.\n",
    "\n",
    "daily_show.map(lambda x: (x[0], 1)).reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "During the map step, we used a lambda function to create a tuple consisting of:\n",
    "key: x[0] (the first value in the list)\n",
    "value: 1 (the integer)\n",
    "\n",
    "Our high-level strategy was to create a tuple with the key representing the year, and the value representing 1. After running the map step, Spark will maintain in memory a list of tuples resembling the following:\n",
    "\n",
    "('YEAR', 1)\n",
    "('1991', 1)\n",
    "('1991', 1)\n",
    "('1991', 1)\n",
    "('1991', 1)\n",
    "...\n",
    "\n",
    "We'd like to reduce that down to:\n",
    "\n",
    "('YEAR', 1)\n",
    "('1991', 4)\n",
    "...\n",
    "\n",
    "reduceByKey(f) combines tuples with the same key using the function we specify, f.\n",
    "\n",
    "To see the results of these two steps, we'll use the take command, which forces lazy code to run immediately. Because tally is an RDD, we can't use Python's len function to find out how many elements are in the collection. Instead, we'll need to use the RDD count() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tally.take(tally.count())\n",
    "\n",
    "\"\"\"Output\n",
    "[('YEAR', 1),\n",
    " ('2013', 166),\n",
    " ('2001', 157),\n",
    " ('2004', 164),\n",
    " ('2000', 169),\n",
    " ('2015', 100),\n",
    " ('2010', 165),\n",
    " ('2006', 161),\n",
    " ('2014', 163),\n",
    " ('2003', 166),\n",
    " ('2002', 159),\n",
    " ('2011', 163),\n",
    " ('2012', 164),\n",
    " ('2008', 164),\n",
    " ('2007', 141),\n",
    " ('2005', 162),\n",
    " ('1999', 166),\n",
    " ('2009', 163)]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike pandas, Spark knows nothing about column headers, and didn't set them aside. We need a way to remove the element ('YEAR', 1) from our collection. We'll need a workaround, though, because RDD objects are immutable once we create them. The only way to remove that tuple is to create a new RDD object that doesn't have it.\n",
    "\n",
    "Spark comes with a filter(f) function that creates a new RDD by filtering an existing one for specific criteria. If we specify a function f that returns a binary value, True or False, the resulting RDD will consist of elements where the function evaluated to True. You can read more about the filter function in the Spark documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Write a function named filter_year that we can use to filter out the element that \n",
    "begins with the text YEAR, instead of an actual year.\"\"\"\n",
    "\n",
    "def filter_year(line):\n",
    "    # logic here\n",
    "    if line[0] == 'YEAR':\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    return True\n",
    "\n",
    "filtered_daily_show = daily_show.filter(lambda line: filter_year(line))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the advantages of DAG (directed acyclic graph) execution of big data algorithms over MapReduce? I know that Apache Spark, Storm, and Tez use the DAG execution model, over MapReduce. Why? Are there any disadvantages?\n",
    "\n",
    "https://www.quora.com/What-are-the-advantages-of-DAG-directed-acyclic-graph-execution-of-big-data-algorithms-over-MapReduce-I-know-that-Apache-Spark-Storm-and-Tez-use-the-DAG-execution-model-over-MapReduce-Why-Are-there-any-disadvantages/answer/Tathagata-Das?share=1&srid=umKP\n",
    "\n",
    "1. Conceptually DAG model is a strict generalization of MapReduce model. DAG-based systems like Spark and Tez that are aware of the whole DAG of operations can do better global optimizations than systems like Hadoop MapReduce which are unaware of the DAG to be executed.\n",
    "\n",
    "2. MapReduce model simply states that distributed computation on a large dataset can be boiled down to two kinds of computation steps - a map step and a reduce step. One pair of map and reduce does one level of aggregation over the data. Complex computations typically require multiple such steps. When you have multiple such steps, it essentially forms a DAG of operations. So a DAG execution model is essentially a generalization of the MapReduce model. (Elaboration of 1 above)\n",
    "\n",
    "3. Computations expressed in Hadoop MapReduce boil down to multiple iterations of: \n",
    "    (i) read data from HDFS, \n",
    "    (ii) apply map and reduce, \n",
    "    (iii) write back to HDFS. \n",
    "    So, Each map-reduce round is completely independent of each other, and Hadoop does not have any global knowledge of what MR steps are going to come after each MR. For many iterative algorithms this is inefficient as the data between each map-reduce pair gets written and read from filesystem. Newer systems like Spark and Tez improves performance over Hadoop by considering the whole DAG of map-reduce steps and optimizing it globally (e.g., pipelining consecutive map steps into one, not write intermediate data to HDFS). This prevents writing data back and forth after every reduce.\n",
    "    \n",
    "4. Spark Streaming does not pre-allocate, rather uses the underlying Spark's mechanisms to dynamically allocate tasks to available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To flex Spark's muscles, we'll demonstrate how to chain together a series of data transformations into a pipeline, and observe Spark managing everything in the background. The developers who wrote Spark had this functionality in mind, and optimized it for running tasks in succession.\n",
    "\n",
    "Before Spark came along, running lots of tasks in succession in Hadoop was incredibly time consuming. Hadoop had to write intermediate results to disk, and wasn't aware of the full pipeline. Thanks to its aggressive approach to memory use and well-architected core, Spark improves on Hadoop's turnaround time significantly. \n",
    "\n",
    "In the following code cell, we'll filter out actors for whom the profession is blank, lowercase each profession, generate a histogram of professions, and output the first five tuples in the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_daily_show.filter(lambda line: line[1] != '') \\\n",
    "                   .map(lambda line: (line[1].lower(), 1)) \\\n",
    "                   .reduceByKey(lambda x,y: x+y) \\\n",
    "                   .take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce Patterns implemented in Apache Spark\n",
    "\n",
    "https://mapr.com/blog/mapreduce-design-patterns-implemented-apache-spark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to set up Spark on your own computer and integrate PySpark with Jupyter Notebook? \n",
    "\n",
    "We can use Spark in two modes:\n",
    "\n",
    "1. Local mode - The entire Spark application runs on a single machine. Local mode is what you'll use to prototype Spark code on your own computer. It's also easier to set up.\n",
    "\n",
    "2. Cluster mode - The Spark application runs across multiple machines. Cluster mode is what you'll use when you want to run your Spark application across multiple machines in a cloud environment like Amazon Web Services, Microsoft Azure, or Digital Ocean.\n",
    "\n",
    "For now, we'll walk through the instructions for installing Spark in local mode on Windows, Mac, and Linux. We'll cover how to install Spark in cluster mode as part of the data engineering track.\n",
    "\n",
    "Here's a diagram describing the high-level components you'll be setting up today:\n",
    "\n",
    "![title](sparkpytharch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n",
      "Collecting py4j==0.10.7\n",
      "  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257935 sha256=7c50a53bed32debf1bafeb45e79121995220a9f44c69a095fa4598c332f94762\n",
      "  Stored in directory: C:\\Users\\Bidhan\\AppData\\Local\\pip\\Cache\\wheels\\bf\\db\\04\\61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 2.4.5\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: c:\\users\\bidhan\\anaconda3\\lib\\site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading https://files.pythonhosted.org/packages/b1/c8/e6e1f6a303ae5122dc28d131b5a67c5eb87cbf8f7ac5b9f87764ea1b1e1e/findspark-1.3.0-py2.py3-none-any.whl\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('C:/Users/Bidhan/spark-2.4.5-bin-hadoop2.6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read `recent-grads.csv` in to an RDD.\n",
    "f = sc.textFile('recent-grads.csv')\n",
    "data = f.map(lambda line: line.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Rank,Major_code,Major,Total,Men,Women,Major_category,ShareWomen,Sample_size,Employed,Full_time,Part_time,Full_time_year_round,Unemployed,Unemployment_rate,Median,P25th,P75th,College_jobs,Non_college_jobs,Low_wage_jobs'],\n",
       " ['1,2419,PETROLEUM ENGINEERING,2339,2057,282,Engineering,0.120564344,36,1976,1849,270,1207,37,0.018380527,110000,95000,125000,1534,364,193'],\n",
       " ['2,2416,MINING AND MINERAL ENGINEERING,756,679,77,Engineering,0.101851852,7,640,556,170,388,85,0.117241379,75000,55000,90000,350,257,50'],\n",
       " ['3,2415,METALLURGICAL ENGINEERING,856,725,131,Engineering,0.153037383,3,648,558,133,340,16,0.024096386,73000,50000,105000,456,176,0'],\n",
       " ['4,2417,NAVAL ARCHITECTURE AND MARINE ENGINEERING,1258,1123,135,Engineering,0.107313196,16,758,1069,150,692,40,0.050125313,70000,43000,80000,529,102,0'],\n",
       " ['5,2405,CHEMICAL ENGINEERING,32260,21239,11021,Engineering,0.341630502,289,25694,23170,5180,16697,1672,0.061097712,65000,50000,75000,18314,4440,972'],\n",
       " ['6,2418,NUCLEAR ENGINEERING,2573,2200,373,Engineering,0.144966965,17,1857,2038,264,1449,400,0.177226407,65000,50000,102000,1142,657,244'],\n",
       " ['7,6202,ACTUARIAL SCIENCE,3777,2110,1667,Business,0.441355573,51,2912,2924,296,2482,308,0.095652174,62000,53000,72000,1768,314,259'],\n",
       " ['8,5001,ASTRONOMY AND ASTROPHYSICS,1792,832,960,Physical Sciences,0.535714286,10,1526,1085,553,827,33,0.021167415,62000,31500,109000,972,500,220'],\n",
       " ['9,2414,MECHANICAL ENGINEERING,91227,80320,10907,Engineering,0.119558903,1029,76442,71298,13101,54639,4650,0.057342278,60000,48000,70000,52844,16384,3253']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.tanka.la/2018/09/02/run-your-first-spark-program-using-pyspark-and-jupyter-notebook/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_hamlet = sc.textFile('hamlet.txt')\n",
    "raw_hamlet.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_hamlet = raw_hamlet.map(lambda line: line.split('\\t'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda functions are great for writing quick functions we can pass into PySpark methods with simple logic. They fall short when we need to write more customized logic, though. Thankfully, PySpark lets us define a function in Python first, then pass it in. Any function that returns a sequence of data in PySpark (versus a guaranteed Boolean value, like filter() requires) must use a yield statement to specify the values that should be pulled later.\n",
    "\n",
    "If you're unfamiliar with the yield statement in Python, read this excellent Stack Overflow answer (https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do/231855#231855) on the topic. \n",
    "\n",
    "To summarize, yield is a Python technique that allows the interpreter to generate data on the fly and pull it when necessary, instead of storing it to memory immediately. Because of its unique architecture, Spark takes advantage of this technique to reduce overhead and improve the speed of computations.\n",
    "\n",
    "Spark runs the named function on every element in the RDD and restricts it in scope. Each instance of the function only has access to the object(s) you pass into the function, and the Python libraries available in your environment. If you try to refer to variables outside the scope of the function or import libraries, those actions may cause the computation to crash. That's because Spark compiles the function's code to Java to run on the RDD objects (which are also in Java).\n",
    "\n",
    "Finally, not all functions require us to use yield; only the ones that generate a custom sequence of data do. For map() or filter(), we use return to return a value for every single element in the RDD we're running the functions on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatMap() is different than map() because it doesn't require an output for every element in the RDD. The flatMap() method is useful whenever we want to generate a sequence of values from an RDD.\n",
    "\n",
    "We can't use the map() method for this because it requires a return value for every element in the RDD.\n",
    "\n",
    "Difference between map() and flatMap(): https://www.baeldung.com/java-difference-map-and-flatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamlet_speaks(line):\n",
    "    id = line[0]\n",
    "    speaketh = False\n",
    "    \n",
    "    if \"HAMLET\" in line:\n",
    "        speaketh = True\n",
    "    \n",
    "    if speaketh:\n",
    "        yield id,\"hamlet speaketh!\"\n",
    "\n",
    "hamlet_spoken = split_hamlet.flatMap(lambda x: hamlet_speaks(x))\n",
    "#hamlet_spoken.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a named function filter_hamlet_speaks to pass into filter(). Apply it to split_hamlet to return an RDD with the elements containing the word HAMLET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def filter_hamlet_speaks(line):\n",
    "    if \"HAMLET\" in line:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "hamlet_spoken_lines = split_hamlet.filter(lambda line: filter_hamlet_speaks(line))\n",
    "hamlet_spoken_lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we use an action method, Spark forces the evaluation of lazy code. If we only chain together transformation methods and print the resulting RDD object, we'll see the type of RDD (e.g. a PythonRDD or PipelinedRDD object), but not the elements within it. That's because the computation hasn't actually happened yet.\n",
    "\n",
    "Even though Spark simplifies chaining lots of transformations together, it's good practice to use actions to observe the intermediate RDD objects between those transformations. This will let you know whether your transformations are working the way you expect them to.\n",
    "\n",
    "1. The count() method returns the number of elements in an RDD.\n",
    "\n",
    "2. We've used take() to preview the first few elements of an RDD, similar to the way we've used head() in pandas. But what about returning all of the elements in a collection? We need to do this to write an RDD to a CSV, for example. It's also useful for running some basic Python code over a collection without going through PySpark.\n",
    "\n",
    "3. Running .collect() on an RDD returns a list representation of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoken_count = hamlet_spoken_lines.count()\n",
    "spoken_101 = list(hamlet_spoken_lines.collect())[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope you have a better idea of how to use PySpark to transform a dataset into a format that's better for data analysis. \n",
    "\n",
    "We also learned how to use actions to explore an RDD before chaining another transformation to it.\n",
    "\n",
    "The next mission is a challenge that will test your understanding of Spark, transformations, actions, lambda functions, and the MapReduce paradigm in general. \n",
    "\n",
    "After that challenge, we'll explore Spark DataFrames and how to analyze data using all the techniques we've learned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://training.databricks.com/visualapi.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_hamlet = sc.textFile(\"hamlet.txt\")\n",
    "split_hamlet = raw_hamlet.map(lambda line: line.split('\\t'))\n",
    "split_hamlet.take(5)\n",
    "\n",
    "def format_id(x):\n",
    "    id = x[0].split('@')[1]\n",
    "    results = list()\n",
    "    results.append(id)\n",
    "    if len(x) > 1:\n",
    "        for y in x[1:]:\n",
    "            results.append(y)\n",
    "    return results\n",
    "\n",
    "hamlet_with_ids = split_hamlet.map(lambda line: format_id(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet_with_ids.take(5)\n",
    "\n",
    "real_text = hamlet_with_ids.filter(lambda line: len(line) > 1)\n",
    "hamlet_text_only = real_text.map(lambda line: [l for l in line if l != ''])\n",
    "hamlet_text_only.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet_text_only.take(10)\n",
    "def fix_pipe(line):\n",
    "    results = list()\n",
    "    for l in line:\n",
    "        if l == \"|\":\n",
    "            pass\n",
    "        elif \"|\" in l:\n",
    "            fmtd = l.replace(\"|\", \"\")\n",
    "            results.append(fmtd)\n",
    "        else:\n",
    "            results.append(l)\n",
    "    return results\n",
    "\n",
    "clean_hamlet = hamlet_text_only.map(lambda line: fix_pipe(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark DataFrame is a feature that allows you to create and work with DataFrame objects. As you may have guessed, pandas inspired it.\n",
    "\n",
    "Spark is well known for its ability to process large data sets. Spark DataFrames combine the scale and speed of Spark with the familiar query, filter, and analysis capabilities of pandas. Unlike pandas, which can only run on one computer, Spark can use distributed memory (and disk when necessary) to handle larger data sets and run computations more quickly.\n",
    "\n",
    "Spark DataFrames allow us to modify and reuse our existing pandas code to scale up to much larger data sets. They also have better support for various data formats. We can even use a SQL interface to write distributed SQL queries that query large database systems and other data stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('census_2010.json') as f:\n",
    "    for i in range(4):\n",
    "        print(f.readline())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous missions, we explored reading data into an RDD object. Recall that an RDD is essentially a list of tuples with no enforced schema or structure of any kind. An RDD can have a variable number of elements in each tuple, and combinations of types between tuples.\n",
    "\n",
    "RDDs are useful for representing unstructured data like text. Without them, we'd need to write a lot of custom Python code to interact with such data.\n",
    "\n",
    "We use the SparkContext object to read data into an RDD:\n",
    "\n",
    "raw_data = sc.textFile(\\\"daily_show.tsv\\\")\n",
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "\n",
    "To use the familiar DataFrame query interface from pandas, however, the data representation needs to include rows, columns, and types. Spark's implementation of DataFrames mirrors the pandas implementation, with logic for rows and columns.\n",
    "\n",
    "The Spark SQL class is very powerful. It gives Spark more information about the data structure we're using and the computations we want to perform. Spark uses that information to optimize processes.\n",
    "\n",
    "To take advantage of these features, we'll have to use the SQLContext object to structure external data as a DataFrame, instead of the SparkContext object.\n",
    "\n",
    "We can query Spark DataFrame objects with SQL, which we'll explore in the next mission. The SQLContext class gets its name from this capability.\n",
    "\n",
    "This class allows us to read in data and create new DataFrames from a wide range of sources. It can do this because it takes advantage of Spark's powerful Data Sources API.\n",
    "\n",
    "File Formats\n",
    "\n",
    "JSON, CSV/TSV, XML\n",
    "Parquet, Amazon S3 (cloud storage service)\n",
    "\n",
    "Big Data Systems\n",
    "Hive, Avro, HBase\n",
    "\n",
    "SQL Database Systems\n",
    "MySQL, PostgreSQL\n",
    "\n",
    "Data science organizations often use a wide range of systems to collect and store data, and they're constantly making changes to those systems. Spark DataFrames allow us to interface with different types of data, and ensure that our analysis logic will still work as the data storage mechanisms change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SQLContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# Pass in the SparkContext object `sc`\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "# Read JSON data into a DataFrame object `df`\n",
    "df = sqlCtx.read.json(\"census_2010.json\")\n",
    "\n",
    "# Print the type\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we read data into the SQLContext object, Spark:\n",
    "\n",
    "1. Instantiates a Spark DataFrame object\n",
    "2. Infers the schema from the data and associates it with the DataFrame\n",
    "3. Reads in the data and distributes it across clusters (if multiple clusters are available)\n",
    "4. Returns the DataFrame object\n",
    "\n",
    "We expect the DataFrame Spark created to have the following columns, which were the keys in the JSON data set:\n",
    "\n",
    "age\n",
    "females\n",
    "males\n",
    "total\n",
    "year\n",
    "\n",
    "Spark has its own type system that's similar to the pandas type system. To create a DataFrame, Spark iterates over the data set twice - once to extract the structure of the columns, and once to infer each column's type. Let's use one of the Spark DataFrame instance methods to display the schema for the DataFrame we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlCtx = SQLContext(sc)\n",
    "df = sqlCtx.read.json(\"census_2010.json\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned before, the pandas DataFrame heavily influenced the Spark DataFrame implementation. Here are some of the methods we can find in both:\n",
    "\n",
    "1. agg()\n",
    "2. join()\n",
    "3. sort()\n",
    "4. where()\n",
    "\n",
    "Unlike pandas DataFrames, however, Spark DataFrames are immutable, which means we can't modify existing objects. Most transformations on an object return a new DataFrame reflecting the changes instead. As we discussed in previous missions, Spark's creators deliberately designed immutability into Spark to make it easier to work with distributed data structures.\n",
    "\n",
    "Pandas and Spark DataFrames also have different underlying data structures. Pandas DataFrames are built around Series objects, while Spark DataFrames are built around RDDs. We can perform most of the same computations and transformations on Spark DataFrames that we can on pandas DataFrames, but the styles and methods are somewhat different. We'll explore how to perform common pandas functions with Spark in this mission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5) #only prints the top 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas, we used the head() method to return the first n rows. This is one of the differences between the DataFrame implementations. Instead of returning a nicely formatted table of values, the head() method in Spark returns a list of row objects. Spark needs to return row objects for certain methods, such as head(), collect() and take().\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Use the head() method to return the first five rows in the DataFrame as a list of row objects, and assign the result to the variable first_five.\n",
    "\n",
    "Print the age value for each row object in first_five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can access a row's attributes by the column name using dot notation, \n",
    "# and by position using bracket notation with an index:\n",
    "\n",
    "row_one = df.head(5)[0]\n",
    "# Access value for age\n",
    "row_one.age\n",
    "# Access the first value\n",
    "row_one[0]\n",
    "\n",
    "###\n",
    "\n",
    "first_five = df.head(5)\n",
    "for i in range(5):\n",
    "    print(first_five[i].age)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas, we passed a string into a single pair of brackets ([]) to select an individual column, and passed in a list to select multiple columns:\n",
    "\n",
    "# Pandas DataFrame\n",
    "\n",
    "df['age']\n",
    "\n",
    "df[['age', 'males']]\n",
    "\n",
    "We can still use bracket notation in Spark. We'll need to pass in a list of string objects, though, even when we're only selecting one column.\n",
    "\n",
    "Spark takes advantage of lazy loading with DataFrames, and will only display the results of an operation when we call the show() method. Instead of using bracket notation, we can also use the select() method to select columns:\n",
    "\n",
    "# Spark DataFrame\n",
    "\n",
    "df.select('age')\n",
    "\n",
    "df.select('age', 'males')\n",
    "\n",
    "In the following code cell, we demonstrate how to select and display the age column. Use what you've learned to take this a step farther and select multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['age']].show()\n",
    "df.select('age','males','females').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas, we used Boolean filtering to select only the rows we were interested in. \n",
    "\n",
    "Spark preserves the very same functionality and notation.\n",
    "\n",
    "1. Use the pandas notation for Boolean filtering to select the rows where age is greater than five.\n",
    "2. Assign the resulting DataFrame to the variable five_plus.\n",
    "3. Use the show() method to display five_plus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_plus = df[df['age']>5]\n",
    "five_plus.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the columns in Spark DataFrames with each other, and use the comparison criteria as a filter. \n",
    "\n",
    "For example, to get the rows where the population of males exceeded females in 2010, we'd write the same notation that we would use in pandas.\n",
    "\n",
    "Q. Find all of the rows where females is less than males, and use show() to display the first 20 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['females']<df['males']].show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark DataFrame is fairly new, and the library's still a bit limited. There's no easy way to create a histogram of the data in a column, for example, or a line plot of the values in two columns.\n",
    "\n",
    "To handle some of these shortcomings, we can convert a Spark DataFrame to a pandas DataFrame using the toPandas() method. Converting an entire Spark DataFrame to a pandas DataFrame works just fine for small data sets. For larger ones, though, we'll want to select a subset of the data that's more manageable for pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df.toPandas()\n",
    "pandas_df['total'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have more or less explored the Spark DataFrame, and how to work with its methods to query and analyze data. \n",
    "\n",
    "In the next coming cells, we'll use SQL to interface with DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "Before we can write and run SQL queries, we need to tell Spark to treat the DataFrame as a SQL table. Spark internally maintains a virtual database within the SQLContext object. This object, which we enter as sqlCtx, has methods for registering temporary tables.\n",
    "\n",
    "To register a DataFrame as a table, call the registerTempTable() method on that DataFrame object. This method requires one string parameter, name, that we use to set the table name for reference in our SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "df = sqlCtx.read.json(\"census_2010.json\")\n",
    "df.registerTempTable(\"census2010\")\n",
    "tables = sqlCtx.tableNames()\n",
    "print(tables)\n",
    "\n",
    "sqlCtx.sql(\"select age from census2010\").show()\n",
    "\n",
    "query = 'select males, females from census2010 where age>5 and age<15'\n",
    "sqlCtx.sql(query).show()\n",
    "\n",
    "query = 'select males,females from census2010'\n",
    "sqlCtx.sql(query).describe().show()\n",
    "\n",
    "df_2000 = sqlCtx.read.json(\"census_2000.json\")\n",
    "df_1990 = sqlCtx.read.json(\"census_1990.json\")\n",
    "df_1980 = sqlCtx.read.json(\"census_1980.json\")\n",
    "\n",
    "df_2000.registerTempTable('census2000')\n",
    "df_1990.registerTempTable('census1990')\n",
    "df_1980.registerTempTable('census1980')\n",
    "tables = sqlCtx.tableNames()\n",
    "print(tables)\n",
    "\n",
    "query = \"\"\"\n",
    " select census2010.total, census2000.total\n",
    " from census2010\n",
    " inner join census2000\n",
    " on census2010.age=census2000.age\n",
    "\"\"\"\n",
    "\n",
    "sqlCtx.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've registered the table within sqlCtx, we can start writing and running SQL queries. With Spark SQL, we represent our query as a string and pass it into the sql() method within the SQLContext object. The sql() method requires a single parameter, the query string. Spark will return the query results as a DataFrame object. This means you'll have to use show() to display the results, due to lazy loading.\n",
    "\n",
    "While SQLite requires that queries end with a semi-colon, Spark SQL will actually throw an error if you include it. Other than this difference in syntax, Spark's flavor of SQL is identical to SQLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlCtx.sql(\"select age from census2010\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we used DataFrame methods to find all of the rows where age was greater than 5. If we only wanted to retrieve data from the males and females columns where that criteria were true, we'd need to chain additional operations to the Spark DataFrame. To return the results in descending order instead of ascending order, we'd have to chain another method. The DataFrame methods are quick and powerful for simple queries, but chaining them can be cumbersome for more advanced queries.\n",
    "\n",
    "SQL shines at expressing complex logic in a more compact manner. Let's brush up on SQL by writing a query that expresses more specific criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'select males, females from census2010 where age>5 and age<15'\n",
    "sqlCtx.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the results of SQL queries are DataFrame objects, we can combine the best aspects of both DataFrames and SQL to enhance our workflow. For example, we can write a SQL query that quickly returns a subset of our data as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'select males,females from census2010'\n",
    "sqlCtx.sql(query).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most powerful use cases in SQL is joining tables. Spark SQL takes this a step further by enabling you to run join queries across data from multiple file types. Spark will read any of the file types and formats it supports into DataFrame objects and we can register each of these as tables within the SQLContext object to use for querying.\n",
    "\n",
    "As we mentioned briefly in the previous mission, most data science organizations use a variety of file formats and data storage mechanisms. Spark SQL was built with the industry use cases in mind and enables data professionals to use one common query language, SQL, to interact with lots of different data sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "df = sqlCtx.read.json(\"census_2010.json\")\n",
    "df.registerTempTable('census2010')\n",
    "df_2000 = sqlCtx.read.json(\"census_2000.json\")\n",
    "df_1990 = sqlCtx.read.json(\"census_1990.json\")\n",
    "df_1980 = sqlCtx.read.json(\"census_1980.json\")\n",
    "\n",
    "df_2000.registerTempTable('census2000')\n",
    "df_1990.registerTempTable('census1990')\n",
    "df_1980.registerTempTable('census1980')\n",
    "tables = sqlCtx.tableNames()\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a table for each dataset, we can write join queries to compare values across them. Since we're working with Census data, let's use the age column as the joining column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    " select census2010.total, census2000.total\n",
    " from census2010\n",
    " inner join census2000\n",
    " on census2010.age=census2000.age\n",
    "\"\"\"\n",
    "\n",
    "sqlCtx.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions and operators from SQLite that we've used in the past are available for us to use in Spark SQL:\n",
    "\n",
    "1. COUNT()\n",
    "2. AVG()\n",
    "3. SUM()\n",
    "4. AND\n",
    "5. OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    " select sum(census2010.total), sum(census2000.total), sum(census1990.total)\n",
    " from census2010\n",
    " inner join census2000\n",
    " on census2010.age=census2000.age\n",
    " inner join census1990\n",
    " on census2010.age=census1990.age\n",
    "\"\"\"\n",
    "sqlCtx.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the purpose of Spark SQL?\n",
    "\n",
    "https://www.quora.com/What-is-the-purpose-of-Spark-SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
