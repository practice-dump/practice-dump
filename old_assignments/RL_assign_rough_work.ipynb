{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in c:\\users\\tanmey\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.7.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pygame\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_track(file_name):\n",
    "  track=open(file_name,'r')\n",
    "  temp = track.read() \n",
    "  temp_list = temp.split(\"\\n\") \n",
    "  string_track=[t.replace('F', '5').replace('S', '0').replace('O', '2').replace('X','1') for t in temp_list]\n",
    "  final_track=[]\n",
    "  for each in string_track:\n",
    "    temporary=[]\n",
    "    if len(each)!=0:\n",
    "      for i in each:\n",
    "        temporary.append(int(i))\n",
    "      final_track.append(temporary)\n",
    "  return np.array(final_track)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have replaced:\n",
    "\n",
    "F by 5\n",
    "\n",
    "S by 0\n",
    "\n",
    "O by 2\n",
    "\n",
    "X by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "track=get_track('track-1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=track.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "     #HELPFUL FUNCTIONS\n",
    "    def get_start_line(self):\n",
    "        '''\n",
    "        Gets start line\n",
    "        '''\n",
    "        self.start_line = np.array([np.array([x-1,j]) for j in range(y) if track[x-1,j] == 0])\n",
    "        \n",
    "    def get_finish_line(self):\n",
    "        '''\n",
    "        Gets finish line\n",
    "        '''\n",
    "        self.finish_line = np.array([np.array([0,i]) for i in range(COLS) if track[0,i] == 5])\n",
    "        \n",
    "    #CONSTRUCTOR\n",
    "    def __init__(self):\n",
    "        '''\n",
    "            racetrack: 2 dimensional numpy array\n",
    "            Q(s,a): 5 dimensional numpy array\n",
    "            C(s,a): 5 dimensional numpy array\n",
    "            pi: target policy\n",
    "            hyperparameters like epsilon\n",
    "            episode to be an empty list\n",
    "        '''\n",
    "        self.get_start_line()\n",
    "        self.get_finish_line()\n",
    "        self.load_Q_vals()\n",
    "        self.load_C_vals()\n",
    "        self.load_pi()\n",
    "        self.load_rewards()\n",
    "        self.epsilon = 0.1\n",
    "        self.gamma = 0.99\n",
    "        self.episode = dict({'S':[],'A':[],'probs':[],'R':[None]})\n",
    "        self.load_racetrack()\n",
    "        \n",
    "    def save_rewards(self,filename = 'rewards'):\n",
    "        '''\n",
    "        saves self.rewards in rewards.npy file\n",
    "        '''\n",
    "        self.rewards = np.array(self.rewards)\n",
    "        np.save(filename,self.rewards)\n",
    "        self.rewards = list(self.rewards)\n",
    "        \n",
    "    def load_rewards(self):\n",
    "        '''\n",
    "        loads rewards from rewards.npy file\n",
    "        '''\n",
    "        self.rewards = list(np.load('rewards.npy'))\n",
    "        \n",
    "    def save_pi(self,filename = 'pi.npy'):\n",
    "        '''\n",
    "        saves self.pi in pi.npy file\n",
    "        '''\n",
    "        np.save(filename,self.pi)\n",
    "        \n",
    "    def load_pi(self):\n",
    "        '''\n",
    "        loads pi from pi.npy file\n",
    "        '''\n",
    "        self.pi = np.load('pi.npy')\n",
    "        \n",
    "    def save_C_vals(self,filename = 'C_vals.npy'):\n",
    "        '''\n",
    "        saves self.C_vals in C_vals.npy file\n",
    "        '''\n",
    "        np.save(filename,self.C_vals)\n",
    "        \n",
    "    def load_C_vals(self):\n",
    "        '''\n",
    "        loads C_vals from C_vals.npy file\n",
    "        '''\n",
    "        self.C_vals = np.load('C_vals.npy')\n",
    "        \n",
    "    def save_Q_vals(self,filename = 'Q_vals.npy'):\n",
    "        '''\n",
    "        saves self.Q_vals in Q_vals.npy file\n",
    "        '''\n",
    "        np.save(filename,self.Q_vals)\n",
    "        \n",
    "    def load_Q_vals(self):\n",
    "        '''\n",
    "        loads Q_vals from Q_vals.npy file\n",
    "        '''\n",
    "        self.Q_vals = np.load('Q_vals.npy')\n",
    "        \n",
    "    def save_racetrack(self,filename = 'racetrack.npy'):\n",
    "        '''\n",
    "        saves self.racetrack in racetrack.npy file\n",
    "        '''\n",
    "        np.save(filename,self.racetrack)\n",
    "        \n",
    "    def load_racetrack(self):\n",
    "        '''\n",
    "        loads racetrack from racetrack.npy file\n",
    "        '''\n",
    "        self.racetrack = track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    #HELPFUL FUNCTIONS\n",
    "    \n",
    "    def get_new_state(self, state, action):\n",
    "        '''\n",
    "        Get new state after applying action on this state\n",
    "        Assumption: The car keeps on moving with the current velocity and then action is applied to \n",
    "        change the velocity\n",
    "        '''\n",
    "        new_state = state.copy()\n",
    "        new_state[0] = state[0] + state[2]\n",
    "        new_state[1] = state[1] + state[3]\n",
    "        new_state[2] = state[2] + action[0]\n",
    "        new_state[3] = state[3] + action[1]\n",
    "        return new_state\n",
    "    \n",
    "    def select_randomly(self,NUMPY_ARR):\n",
    "        '''\n",
    "        Returns a value uniform randomly from NUMPY_ARR\n",
    "        Here NUMPY_ARR should be 1 dimensional\n",
    "        '''\n",
    "        return np.random.choice(NUMPY_ARR)\n",
    "    \n",
    "    def set_zero(NUMPY_ARR):\n",
    "        '''\n",
    "        Returns NUMPY_ARR after making zero all the elements in it\n",
    "        '''\n",
    "        NUMPY_ARR[:] = 0\n",
    "        return NUMPY_ARR\n",
    "    \n",
    "    def is_finish_line_crossed(self, state, action):\n",
    "        '''\n",
    "        Returns True if the car crosses the finish line\n",
    "                False otherwise\n",
    "        '''\n",
    "        new_state = self.get_new_state(state, action)\n",
    "        old_cell, new_cell = state[0:2], new_state[0:2]\n",
    "        \n",
    "        '''\n",
    "        new_cell's row index will be less\n",
    "        '''\n",
    "        rows = np.array(range(new_cell[0],old_cell[0]+1))\n",
    "        cols = np.array(range(old_cell[1],new_cell[1]+1))\n",
    "        fin = set([tuple(x) for x in self.data.finish_line])\n",
    "        row_col_matrix = [(x,y) for x in rows for y in cols]\n",
    "        intersect = [x for x in row_col_matrix if x in fin]\n",
    "        \n",
    "        return len(intersect) > 0\n",
    "    \n",
    "    def is_out_of_track(self, state, action):\n",
    "        '''\n",
    "        Returns True if the car goes out of track if action is taken on state\n",
    "                False otherwise\n",
    "        '''\n",
    "        new_state = self.get_new_state(state, action)\n",
    "        old_cell, new_cell = state[0:2], new_state[0:2]\n",
    "        \n",
    "        if new_cell[0] < 0 or new_cell[0] >= ROWS or new_cell[1] < 0 or new_cell[1] >= COLS:\n",
    "            return True\n",
    "        elif self.has_collided(new_cell,old_cell):\n",
    "            return True\n",
    "        else:\n",
    "            return self.data.racetrack[tuple(new_cell)] == -1\n",
    "    \n",
    "    #CONSTRUCTOR\n",
    "    def __init__(self, data):\n",
    "        '''\n",
    "        initialize step_count to be 0\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.step_count = 0\n",
    "    \n",
    "    #MEMBER FUNCTIONS\n",
    "    \n",
    "    def reset(self):\n",
    "        self.data.episode = dict({'S':[],'A':[],'probs':[],'R':[None]})\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def start(self):\n",
    "        '''\n",
    "        Makes the velocity of the car to be zero\n",
    "        Returns the randomly selected start state.\n",
    "        '''\n",
    "        state = np.zeros(4,dtype='int')\n",
    "        state[0] = ROWS-1\n",
    "        state[1] = self.select_randomly(self.data.start_line[:,1])\n",
    "        '''\n",
    "        state[2] and state[3] are already zero\n",
    "        '''\n",
    "        return state\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        '''\n",
    "        Returns the reward and new state when action is taken on state\n",
    "        Checks the following 2 cases maintaining the order:\n",
    "            1. car finishes race by crossing the finish line\n",
    "            2. car goes out of track\n",
    "        Ends the episode by returning reward as None and state as usual (which will be terminating)\n",
    "        '''\n",
    "        self.data.episode['A'].append(action)\n",
    "        reward = -1\n",
    "        reward_obs = -10\n",
    "        reward_finish = 10\n",
    "        \n",
    "        if (self.is_finish_line_crossed(state, action)):\n",
    "            new_state = self.get_new_state(state, action)\n",
    "            \n",
    "            self.data.episode['R'].append(reward_finish)\n",
    "            self.data.episode['S'].append(new_state)\n",
    "            self.step_count += 1\n",
    "            \n",
    "            return None, new_state\n",
    "            \n",
    "        elif (self.is_out_of_track(state, action)):\n",
    "            self.data.episode['R'].append(reward_obs)\n",
    "            new_state = self.start()\n",
    "        else:\n",
    "            new_state = self.get_new_state(state, action)\n",
    "        \n",
    "        self.data.episode['R'].append(reward)\n",
    "        self.data.episode['S'].append(new_state)\n",
    "        self.step_count += 1\n",
    "        \n",
    "        return reward, new_state\n",
    "    \n",
    "    def has_collided(self, new_state,old_state):  \n",
    "        sub_track=self.data.racetrack[min(old_state[0],new_state[0]):max(old_state[0],new_state[0])+1,min(old_state[1],new_state[1]):max(old_state[1],new_state[1])+1]\n",
    "        if -1 in sub_track:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    #HELPFUL FUNCTIONS\n",
    "    def possible_actions(self, velocity):\n",
    "        '''\n",
    "        *** Performs two tasks, can be split up ***\n",
    "        Universe of actions:  α = [(-1,-1),(-1,0),(0,-1),(-1,1),(0,0),(1,-1),(0,1),(1,0),(1,1)]\n",
    "                            \n",
    "        Uses constraints to filter out invalid actions given the velocity\n",
    "        \n",
    "        0 <= v_x < 5\n",
    "        0 <= v_y < 5\n",
    "        v_x and v_y cannot be made both zero (you can't take an action which would make them zero simultaneously)\n",
    "        Returns list of possible actions given the velocity\n",
    "        '''\n",
    "        alpha = [(-1,-1),(-1,0),(0,-1),(-1,1),(0,0),(1,-1),(0,1),(1,0),(1,1)]\n",
    "        alpha = [np.array(x) for x in alpha]\n",
    "\n",
    "        beta = []\n",
    "        for i,x in zip(range(9),alpha):\n",
    "            new_vel = np.add(velocity,x)\n",
    "            if (new_vel[0] < 5) and (new_vel[0] >= 0) and (new_vel[1] < 5) and (new_vel[1] >= 0) and ~(new_vel[0] == 0 and new_vel[1] == 0):\n",
    "                beta.append(i)\n",
    "        beta = np.array(β)\n",
    "        \n",
    "        return beta\n",
    "    \n",
    "    def map_to_1D(self,action):\n",
    "        alpha = [(-1,-1),(-1,0),(0,-1),(-1,1),(0,0),(1,-1),(0,1),(1,0),(1,1)]\n",
    "        for i,x in zip(range(9),alpha):\n",
    "            if action[0]==x[0] and action[1]==x[1]:\n",
    "                return i\n",
    "    \n",
    "    def map_to_2D(self,action):\n",
    "        alpha = [(-1,-1),(-1,0),(0,-1),(-1,1),(0,0),(1,-1),(0,1),(1,0),(1,1)]\n",
    "        return alpha[action]\n",
    "    \n",
    "    #CONSTRUCTOR\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_action(self, state, policy):\n",
    "        '''\n",
    "        Returns action given state using policy\n",
    "        '''\n",
    "        return self.map_to_2D(policy(state, self.possible_actions(state[2:4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Monte_Carlo_Control:\n",
    "    \n",
    "    #HELPFUL FUNCTIONS\n",
    "    \n",
    "    def evaluate_target_policy(self):\n",
    "        env.reset()\n",
    "        state = env.start()\n",
    "        self.data.episode['S'].append(state)\n",
    "        rew = -1\n",
    "        while rew!=None:\n",
    "            action = agent.get_action(state,self.generate_target_policy_action)\n",
    "            rew, state = env.step(state,action)\n",
    "            \n",
    "        self.data.rewards.append(sum(self.data.episode['R'][1:]))\n",
    "        \n",
    "    \n",
    "    def plot_rewards(self):\n",
    "        ax, fig = plt.subplots(figsize=(30,15))\n",
    "        x = np.arange(1,len(self.data.rewards)+1)\n",
    "        plt.plot(x*10, self.data.rewards, linewidth=0.5, color = '#BB8FCE')\n",
    "        plt.xlabel('Episode number', size = 20)\n",
    "        plt.ylabel('Reward',size = 20)\n",
    "        plt.title('Plot of Reward vs Episode Number',size=20)\n",
    "        plt.xticks(size=20)\n",
    "        plt.yticks(size=20)\n",
    "        plt.savefig('RewardGraph.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def save_your_work(self):\n",
    "        self.data.save_Q_vals()\n",
    "        self.data.save_C_vals()\n",
    "        self.data.save_pi()\n",
    "        self.data.save_rewards()\n",
    "    \n",
    "    def determine_probability_behaviour(self, state, action, possible_actions):\n",
    "        best_action = self.data.pi[tuple(state)]\n",
    "        num_actions = len(possible_actions)\n",
    "        \n",
    "        if best_action in possible_actions:\n",
    "            if action == best_action:\n",
    "                prob = 1 - self.data.epsilon + self.data.epsilon/num_actions\n",
    "            else:\n",
    "                prob = self.data.epsilon/num_actions\n",
    "        else:\n",
    "            prob = 1/num_actions\n",
    "        \n",
    "        self.data.episode['probs'].append(prob)\n",
    "    \n",
    "    def generate_target_policy_action(self, state, possible_actions):\n",
    "        '''\n",
    "        Returns target policy action, takes state and\n",
    "        returns an action using this policy\n",
    "        '''\n",
    "        if self.data.pi[tuple(state)] in possible_actions:\n",
    "            action = self.data.pi[tuple(state)]\n",
    "        else:\n",
    "            action = np.random.choice(possible_actions)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def generate_behavioural_policy_action(self, state, possible_actions):\n",
    "        '''\n",
    "        Returns behavioural policy action\n",
    "        which would be ε-greedy π policy, takes state and\n",
    "        returns an action using this ε-greedy π policy\n",
    "        '''\n",
    "        if np.random.rand() > self.data.epsilon and self.data.pi[tuple(state)] in possible_actions:\n",
    "            action = self.data.pi[tuple(state)]\n",
    "        else:\n",
    "            action = np.random.choice(possible_actions)\n",
    "        \n",
    "        self.determine_probability_behaviour(state, action, possible_actions)\n",
    "    \n",
    "        return action\n",
    "    \n",
    "    #CONSTRUCTOR\n",
    "    def __init__(self, data):\n",
    "        '''\n",
    "        Initialize, for all s ∈ S, a ∈ A(s):\n",
    "            data.Q(s, a) ← arbitrary (done in Data)\n",
    "            data.C(s, a) ← 0 (done in Data)\n",
    "            π(s) ← argmax_a Q(s,a) \n",
    "            (with ties broken consistently) \n",
    "            (some consistent approach needs to be followed))\n",
    "        '''\n",
    "        self.data = data\n",
    "        for i in range(ROWS):\n",
    "            for j in range(COLS):\n",
    "                if self.data.racetrack[i,j]!=-1:\n",
    "                    for k in range(5):\n",
    "                        for l in range(5):\n",
    "                            self.data.pi[i,j,k,l] = np.argmax(self.data.Q_vals[i,j,k,l])\n",
    "    \n",
    "    def control(self,env,agent):\n",
    "        '''\n",
    "        Performs MC control using episode list [ S0 , A0 , R1, . . . , ST −1 , AT −1, RT , ST ]\n",
    "        G ← 0\n",
    "        W ← 1\n",
    "        For t = T − 1, T − 2, . . . down to 0:\n",
    "            G ← γ*G + R_t+1\n",
    "            C(St, At ) ← C(St,At ) + W\n",
    "            Q(St, At ) ← Q(St,At) + (W/C(St,At))*[G − Q(St,At )]\n",
    "            π(St) ← argmax_a Q(St,a) (with ties broken consistently)\n",
    "            If At != π(St) then exit For loop\n",
    "            W ← W * (1/b(At|St))        \n",
    "        '''\n",
    "        env.reset()\n",
    "        state = env.start()\n",
    "        self.data.episode['S'].append(state)\n",
    "        rew = -1\n",
    "        while rew!=None:\n",
    "            action = agent.get_action(state,self.generate_behavioural_policy_action)\n",
    "            rew, state = env.step(state,action)\n",
    "            print(rew,\" \",action)\n",
    "        \n",
    "        G = 0\n",
    "        W = 1\n",
    "        T = env.step_count\n",
    "        print(\"T: \",T)\n",
    "        for t in range(T-1,-1,-1):\n",
    "            G = data.gamma * G + self.data.episode['R'][t+1]\n",
    "            S_t = tuple(self.data.episode['S'][t])\n",
    "            A_t = agent.map_to_1D(self.data.episode['A'][t])\n",
    "            \n",
    "            S_list = list(S_t)\n",
    "            S_list.append(A_t)\n",
    "            SA = tuple(S_list)\n",
    "            \n",
    "            self.data.C_vals[SA] += W\n",
    "            self.data.Q_vals[SA] += (W*(G-self.data.Q_vals[SA]))/(self.data.C_vals[SA])           \n",
    "            self.data.π[S_t] = np.argmax(self.data.Q_vals[S_t])\n",
    "            if A_t!=self.data.pi[S_t]:\n",
    "                break\n",
    "            W /= self.data.episode['probs'][t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    \n",
    "    #HELPFUL FUNCTIONS\n",
    "    \n",
    "    def visualize_episode():\n",
    "        for i in range(self.data.episode['S']):\n",
    "            vis.visualize_racetrack(i)\n",
    "    \n",
    "    def create_window(self):\n",
    "        '''\n",
    "        Creates window and assigns self.display variable\n",
    "        '''\n",
    "        self.display = pygame.display.set_mode((self.width, self.height))\n",
    "        pygame.display.set_caption(\"Racetrack\")\n",
    "    \n",
    "    def setup(self):\n",
    "        '''\n",
    "        Does things which occur only at the beginning\n",
    "        '''\n",
    "        self.cell_edge = 25\n",
    "        self.width = COLS*self.cell_edge\n",
    "        self.height = ROWS*self.cell_edge\n",
    "        self.create_window()\n",
    "        self.window = True\n",
    "\n",
    "    def close_window(self):\n",
    "        self.window = False\n",
    "        pygame.quit()\n",
    "\n",
    "    def draw(self, state = np.array([])):\n",
    "        self.display.fill(0)\n",
    "        for i in range(ROWS):\n",
    "            for j in range(COLS):\n",
    "                if self.data.racetrack[i,j]!=1:\n",
    "                    if self.data.racetrack[i,j] == 2:\n",
    "                        color = (255,0,0)\n",
    "                    elif self.data.racetrack[i,j] == 0:\n",
    "                        color = (255,255,0)\n",
    "                    elif self.data.racetrack[i,j] == 5:\n",
    "                        color = (0,255,0)\n",
    "                    pygame.draw.rect(self.display,color,((j*self.cell_edge,i*self.cell_edge),(self.cell_edge,self.cell_edge)),1)\n",
    "        \n",
    "        if len(state)>0:\n",
    "            pygame.draw.rect(self.display,(255,255,255),((state[1]*self.cell_edge,state[0]*self.cell_edge),(self.cell_edge,self.cell_edge)),0)\n",
    "        \n",
    "        pygame.display.update()\n",
    "        \n",
    "        global count\n",
    "        \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.loop = False\n",
    "                self.close_window()\n",
    "                return 'stop'\n",
    "            elif event.type == pygame.KEYDOWN and event.key == pygame.K_SPACE:\n",
    "                pygame.image.save(vis.display, str(count)+'.png')\n",
    "                count += 1\n",
    "                self.loop = False\n",
    "                \n",
    "        return None\n",
    "        \n",
    "    def visualize_racetrack(self, state = np.array([])):\n",
    "        '''\n",
    "        Draws Racetrack in a pygame window\n",
    "        '''\n",
    "        if self.window == False:\n",
    "            self.setup()\n",
    "        self.loop = True\n",
    "        while(self.loop):\n",
    "            ret = self.draw(state)\n",
    "            if ret!=None:\n",
    "                return ret\n",
    "    \n",
    "    #CONSTRUCTOR\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.window = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "env = Environment(data)\n",
    "mcc = Monte_Carlo_Control(data)\n",
    "vis = Visualizer(data)\n",
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = 50\n",
    "S = sum(data.rewards[:ch])/ch\n",
    "\n",
    "R = []\n",
    "\n",
    "for i in range(ch,len(data.rewards)):\n",
    "    R.append(S)\n",
    "    S *= ch\n",
    "    S += data.rewards[i]\n",
    "    S -= data.rewards[i-ch]\n",
    "    S /= ch\n",
    "\n",
    "ax, fig = plt.subplots(figsize=(60,30))\n",
    "x = np.arange(1,len(R)+1)\n",
    "plt.plot(x*10, R, linewidth=1, color = '#BB8FCE')\n",
    "plt.xlabel('Episode number', size = 40)\n",
    "plt.ylabel('Reward',size = 40)\n",
    "plt.title('Plot of Reward vs Episode Number',size=40)\n",
    "plt.xticks(size=40)\n",
    "plt.yticks(size=40)\n",
    "plt.savefig('RewardGraph2.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Q_vals = np.random.rand(ROWS,COLS,5,5,9)*400 - 500\n",
    "\n",
    "data.rewards = []\n",
    "\n",
    "data.C_vals = np.zeros((ROWS,COLS,5,5,9))\n",
    "\n",
    "data.π = np.zeros((ROWS,COLS,5,5),dtype='int')\n",
    "\n",
    "\n",
    "data.save_Q_vals()\n",
    "data.save_C_vals()\n",
    "data.save_rewards()\n",
    "data.save_π()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "state = env.start()\n",
    "mcc.data.episode['S'].append(state)\n",
    "rew = -1\n",
    "while rew!=None:\n",
    "    action = agent.get_action(state,mcc.generate_target_policy_action)\n",
    "    rew, state = env.step(state,action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.episode['S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"press space\")    \n",
    "for i in data.episode['S']:\n",
    "    print(\"press space\")\n",
    "    time.sleep(0.4)\n",
    "    if vis.visualize_racetrack(i) == 'stop':\n",
    "        break\n",
    "vis.close_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "filenames = []\n",
    "\n",
    "for i in range(219):\n",
    "    filenames.append(str(i)+'.png')\n",
    "\n",
    "images = []\n",
    "for filename in filenames:\n",
    "    images.append(imageio.imread(filename))\n",
    "imageio.mimsave('movie.gif', images, duration = 0.25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
